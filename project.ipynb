{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Data Science - Computer Labs 2024-2025\n",
    "\n",
    "Professor: Francois PORTIER\n",
    "Student: Ba-Khuong DANG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "**Simulation set up**\n",
    "\n",
    "I will illustrat the blessing of independence with regard to the variance of the mean of sequence of random variables. \n",
    "\n",
    "Let $Z$ be a random variable and $Z_i$ is the sequence of realized value of $Z$, with $i\\leq1$.\n",
    "\n",
    "Sample mean of $Z$ is given by: $n^{-1}\\sum_{i=1}^n{Z_i}$\n",
    "\n",
    "The variance of empirical mean will have variance equal to: \n",
    "\n",
    "$$\\mathbb{E}\\left[\\left(n^{-1} \\sum_{i=1}^n Z_i - \\mathbb{E}[Z]\\right)^2\\right] = \\frac{\\text{Var}(Z)}{n}=\\|\\hat{\\mu}-\\mu\\|$$\n",
    "\n",
    "This represent the L2 distance between the expected value (true value) and the empirical mean value of $\\Z$. \n",
    "- One advantage of this theorem is that it is always true and does not depend on value of $n$ (sample size), which makes it a reliable measurement. \n",
    "- The disadvantage of this is that it only shows 1 moment of random variable, which cannot be used to fully characterize $\\Z$ \n",
    "\n",
    "**Python implement**\n",
    "\n",
    "I will use `numpy` random generator to generate a sequence of random variable - with predefined mean and variance. \n",
    "\n",
    "To calculate the expected value of $\\mathbb{E}\\|\\hat{\\mu}-\\mu\\|$, I will use Monte Carlo simulation with large random sampling to estimate the results. Notice that since $Z_i$ is a random variable, $\\hat{\\mu}$ is also a random variable so we can apply this technique to approximate expected value of $\\mathbb{E}[\\hat{\\mu}]$\n",
    "\n",
    "Additionally, to highlight the independence property, I will simulate the results using different probability distributions and varying sample sizes of $Z$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2796)\n",
    "\n",
    "\n",
    "# Function for generate Normal Distribution\n",
    "def z_norm(mu, sigma, n):\n",
    "    \"\"\"\n",
    "    Generate sequence of rv with Normal distribution\n",
    "    Returns:\n",
    "        L2 norm distance with the true mean\n",
    "    \"\"\"\n",
    "    Z = np.random.normal(mu, sigma, n)\n",
    "    return (np.mean(Z) - mu) ** 2\n",
    "\n",
    "\n",
    "# Function for generate Exponential Distribution\n",
    "def z_exp(scale, n):\n",
    "    \"\"\"\n",
    "    Generate sequence of rv with Exponential distribution\n",
    "    Returns:\n",
    "        L2 norm distance with the true mean\n",
    "    \"\"\"\n",
    "    Z = np.random.exponential(scale, n)\n",
    "    return (np.mean(Z) - scale) ** 2\n",
    "\n",
    "\n",
    "# Function to calculate Expected value of variance\n",
    "def simulation(nb_simu, dist=\"norm\", *para):\n",
    "    \"\"\"\n",
    "    Monte-Carlo simulation to calculate E[mu_hat - mu]\n",
    "    Returns:\n",
    "        Expected value for L2_norm\n",
    "    \"\"\"\n",
    "    if dist == \"norm\":\n",
    "        result = np.array([z_norm(*para) for _ in range(nb_simu)])\n",
    "    elif dist == \"exp\":\n",
    "        result = np.array([z_exp(*para) for _ in range(nb_simu)])\n",
    "\n",
    "    return np.mean(result)\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "mu_range = np.random.randint(0, 5, 10)  # Param for Normal ditribution\n",
    "sigma_range = np.random.randint(1, 10, 10)  # Param for Normal distribution\n",
    "scale_range = np.random.randint(1, 5, 10)  # Param for Exponential distribution\n",
    "n_range = np.arange(50, 100, 10)  # Sample size\n",
    "nb_simu = 5_000  # Number of MonteCarlo simulation\n",
    "\n",
    "\n",
    "# Compute Expectation of L2 norm for each set of parameters\n",
    "# Normal distribution sample\n",
    "E_l2_norm = np.array(\n",
    "    [\n",
    "        simulation(nb_simu, \"norm\", mu, sigma, n)\n",
    "        for mu, sigma, n in zip(mu_range, sigma_range, n_range)\n",
    "    ]\n",
    ")\n",
    "var_n_norm = np.array(\n",
    "    [sigma**2 / n for mu, sigma, n in zip(mu_range, sigma_range, n_range)]\n",
    ")\n",
    "\n",
    "# Normal distribution sample\n",
    "E_l2_exp = np.array(\n",
    "    [simulation(nb_simu, \"exp\", scale, n) for scale, n in zip(scale_range, n_range)]\n",
    ")\n",
    "var_n_exp = np.array([scale**2 / n for scale, n in zip(scale_range, n_range)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(\n",
    "    E_l2_norm, var_n_norm, s=25, c=\"blue\", alpha=0.6, label=\"Normal Distribution\"\n",
    ")  # Plot for normal distribution\n",
    "ax.scatter(\n",
    "    E_l2_exp, var_n_exp, s=25, c=\"red\", alpha=0.6, label=\"Exponential Distribution\"\n",
    ")  # Plot for exponential distribution\n",
    "ax.axline((0, 0), slope=1, c=\"green\", linestyle=\"--\", alpha=0.6, label=\"y=x\")\n",
    "ax.set_xlabel(\n",
    "    r\"$E\\left[\\left(n^{-1}\\sum_{i=1}^{n} Z_i - E\\left[Z\\right]\\right)^2\\right]$\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax.set_ylabel(r\"$\\frac{\\text{var}(Z)}{n}$\", fontsize=14)\n",
    "ax.set_title(\"Variance of sample mean\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "**Simulation set up** \n",
    "\n",
    "I will simulate the effect of **central limit theorem (CLT)** for sequence of random variable, using the same set up sequence as in exercise 1:\n",
    "\n",
    "- $Z$ is a random variable and $Z_i$ is the sequence of realized value of $Z$, with $i\\leq1$.\n",
    "- Empirical mean of $Z$ is: $n^{-1}\\sum_{i=1}^n{Z_i}$\n",
    "\n",
    "Central Limit Theorem states than with large number of sample $(n\\to\\infty)$, the distribution of empirical sample mean will converge to a normal distribution with mean equals true mean $(\\mathbb{E}[Z])$. Mathematically, we can express:\n",
    "\n",
    "$$\\sqrt{n}(\\left(n^{-1} \\sum_{i=1}^n Z_i - \\mathbb{E}[Z]\\right)) \\to \\mathcal{N}(0, \\text{Var}(Z))$$\n",
    "\n",
    "Unlike the $L_2$ error, CLT only holds for large sample size. The advantage of this theorem is that it fully characterize the distribution. Also, CLT holds for any type of distribution of random variable $Z$\n",
    "\n",
    "**Python implementation**\n",
    "\n",
    "Similar to exercise 1, I will implement this theorem using `numpy` random generator, and I use Monte-Carlo simulation method to generate and plot the distribution of $Z$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2796)\n",
    "\n",
    "\n",
    "# Function for generate Normal Distribution\n",
    "def z_norm(mu, sigma, n):\n",
    "    \"\"\"\n",
    "    Generate sequence of rv with Normal distribution\n",
    "    Returns:\n",
    "        Sequence of normal random variable and its pdf\n",
    "    \"\"\"\n",
    "    # x = np.linspace(mu - 4*sigma, mu + 4*sigma, n)  # Range for x values\n",
    "    x = np.random.normal(mu, sigma, n)\n",
    "    x = np.sort(x)\n",
    "    y = norm.pdf(x, mu, sigma)  # pdf of x\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Function to generate Binomial Distribution\n",
    "def z_binom(trial, p, n):\n",
    "    \"\"\"\n",
    "    Generate sequence of rv with Binomial distribution\n",
    "    Returns:\n",
    "        Sample mean that is centered.\n",
    "    \"\"\"\n",
    "    Z = np.random.binomial(trial, p, n)\n",
    "    return (np.mean(Z) - trial * p) * np.sqrt(n)\n",
    "\n",
    "\n",
    "# Set up parametes\n",
    "nb_simu = 1_000  # Number of simulation\n",
    "trial, p, n = 10, 0.5, 500\n",
    "mu = 0\n",
    "sigma = (trial * p) * (1 - p)\n",
    "\n",
    "# Generate Normal Distribution for comparison\n",
    "x_norm, y_norm = z_norm(0, np.sqrt(sigma), 1000)\n",
    "\n",
    "# Set up subplot grid (2x2) for each value of n\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for idx, n in enumerate([5, 50, 100, 250]):\n",
    "    rv_simu = np.array([z_binom(trial, p, n) for i in range(nb_simu)])\n",
    "    # Plot Z distribution and Normal distribution\n",
    "    axs[idx].hist(\n",
    "        rv_simu,\n",
    "        bins=30,\n",
    "        density=True,\n",
    "        color=\"blue\",\n",
    "        edgecolor=\"blue\",\n",
    "        alpha=0.5,\n",
    "        label=f\"Sample mean with n={n}\",\n",
    "    )\n",
    "    axs[idx].plot(x_norm, y_norm, color=\"red\", label=\"Normal Distribution\")\n",
    "    axs[idx].legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that as the sample size $n$ increases, the distribution of the sample mean $\\hat{\\mu}$ approaches a normal distribution, regardless of the original distribution of $Z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "**Simulation set up** \n",
    "\n",
    "I will generate a simulate data for a multinomial logistic regression model. Our model will have 8 predictors $(p = 8)$ and the depedent variable will have 2 categories $( 0 or 1)$. \n",
    "\n",
    "- $X$ is a random vector in $\\R^8$ that follows a Gaussian distribution $X \\sim \\mathcal{N}(0, I_p) with $\\I_p$ is identity matrix. That means our predictors are i.i.d with mean $0$\n",
    "\n",
    "- $Y$ follows a Bernoulli distribution with pdf:\n",
    "$$Y \\sim \\mathcal{B}(p(X))$$\n",
    "\n",
    "- The parameter $p$ (the probabily of $Y = 1$) is defined by function of $x_i$:  \n",
    "$$p(x) = \\frac{\\exp(x_1 + x_2^2 )}{1 + \\exp(x1 + x_2^2 )}$$\n",
    "\n",
    "- Our sample size is $n = 3000$.\n",
    "\n",
    "**Python Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prequisition\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Generate multivariate random variable X ~ N(0, I_p)\n",
    "def generate_data(seed, n, p):\n",
    "    \"\"\"\n",
    "    Generate random data\n",
    "    Returns:\n",
    "    X: (n, p) multinormial rv with n observations and p features\n",
    "    Y: (n, ) binomial rv\n",
    "    \"\"\"\n",
    "    mean = np.zeros(p)\n",
    "    cov = np.identity(p)\n",
    "\n",
    "    if seed: \n",
    "        np.random.seed(seed)  # set seed for reproducibitity\n",
    "    X = np.random.multivariate_normal(mean=mean, cov=cov, size=n)\n",
    "\n",
    "    # Compute sigmoid function of X p(x) = exp(x1 + x2^2) / (1 + exp(x1 + x2^2))\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    p_x = np.exp(x1 + x2**2) / (1 + np.exp(x1 + x2**2))\n",
    "\n",
    "    # Generate Y ~ Bernoulli(p(x))\n",
    "    Y = np.random.binomial(n=1, p=p_x)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Parameters\n",
    "p = 8  # dimension of the Gaussian vector\n",
    "n = 3_000  # number of samples\n",
    "\n",
    "X, Y = generate_data(seed=2796, n=3000, p=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the density of x_i against Y\n",
    "N, p = X.shape\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize=(10, 12))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    x_i = X[:, i]\n",
    "\n",
    "    sns.kdeplot(x_i[Y == 0], ax=ax[i], color=\"red\", fill=True, label=\"Y = 0\")\n",
    "    sns.kdeplot(x_i[Y == 1], ax=ax[i], color=\"blue\", fill=True, label=\"Y = 1\")\n",
    "\n",
    "    ax[i].set_title(f\"Distribution of X_{i+1} for Y = 0 and Y = 1\")\n",
    "\n",
    "# Show the plot\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=2, fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "In this exercise, I will implement Gradient Descent (GD) and Stochastic Gradient Descent (SGD) algorithm for optimization problem. \n",
    "\n",
    "Our goal is to find the logistic regression estimator, defined as Empirical risk minimizer for risk function: \n",
    "\n",
    "$$\n",
    "\\beta_n \\in \\arg \\min_{\\beta \\in \\mathbb{R}^{p+1}} \\left( -\\sum_{i=1}^{n} \\left( Y_i \\log(q_{\\beta}(X_i)) + (1 - Y_i) \\log(1 - q_{\\beta}(X_i)) \\right) \\right)\n",
    "$$\n",
    "\n",
    "with sigmoid function is:\n",
    "\n",
    "$$\n",
    "q_{\\beta}(x) = \\frac{\\exp(\\beta_0 + \\beta^T x)}{1 + \\exp(\\beta_0 + \\beta^T x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Gradient of Risk function\n",
    "\n",
    "First we need to calculate the Gradient of the risk function $\\hat{R}_{(\\beta)}$ with respect to $\\beta$. We will use the average of risk function to have something more like a probability distribution. \n",
    "\n",
    "Our objective function is:\n",
    "\n",
    "$$\n",
    "\\beta_n \\in \\arg \\min_{\\beta \\in \\mathbb{R}^{p+1}} \\left( - \\frac{1}{n} \\sum_{i=1}^{n} \\left( Y_i \\log(q_{\\beta}(X_i)) + (1 - Y_i) \\log(1 - q_{\\beta}(X_i)) \\right) \\right)\n",
    "$$\n",
    "\n",
    "Notice that we have $\\beta \\in \\mathbb{R}^{9}$ because we include the intercept $\\beta_0$ in our calculation for convenience. For this reason, we will include a column of $1$ in our original $X$ matrix. \n",
    "\n",
    "$$\n",
    "Z := \n",
    "\\begin{bmatrix}\n",
    "1 & x_1^T \\\\\n",
    "1 & x_2^T \\\\\n",
    "... & ... \\\\\n",
    "1 & x_n^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Math derivation**\n",
    "\n",
    "The mathematical derivation we calculated during our class is as follows:\n",
    "$$\n",
    "\\nabla_b \\hat{R}_{(\\beta)} = \\sum_{i=1}^{n} \\left( z_i \\left(Y_i - \\frac{\\exp(\\beta^Tz_i)}{1 + \\exp(\\beta^Tz_i)} \\right)\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "**Update rule**\n",
    "\n",
    "For Gradient Descent, at each point of beta that we visit, we will calculate the gradient of the whole data set. \n",
    "\n",
    "Our update rule for $\\beta_i$ is: \n",
    "\n",
    "**Python implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z, beta):\n",
    "    \"\"\"\n",
    "    Calcuate the sigmoid function at a point Z_i, Y_i\n",
    "    Returns:\n",
    "    sigmoid: array(n, 1) - predict probability y_hat\n",
    "    \"\"\"\n",
    "\n",
    "    logit = Z.dot(beta)\n",
    "\n",
    "    # Control logit to avoid overflow in exp function\n",
    "    # logit = np.clip(logit, -100, 100)\n",
    "\n",
    "    sigmoid_ = np.exp(logit) / (1 + np.exp(logit))\n",
    "\n",
    "    # Set up small epsilon to avoid log(0) = -inf\n",
    "    # epsilon = 1e-10\n",
    "    # sigmoid_ = np.clip(sigmoid_, epsilon, 1 - epsilon)\n",
    "\n",
    "    return sigmoid_\n",
    "\n",
    "\n",
    "def risk_beta(Z, Y, beta):\n",
    "    \"\"\"\n",
    "    Calcuate the empirical risk at a point Z_1, Y_1\n",
    "    Returns:\n",
    "    risk_beta: scalar - risk valuated at current beta value.\n",
    "    \"\"\"\n",
    "\n",
    "    Yhat = sigmoid(Z, beta)\n",
    "\n",
    "    risk_beta = -(\n",
    "        Y.T.dot(np.log(Yhat)) + (1 - Y).T.dot(np.log(1 - Yhat))\n",
    "    )  # / Y.shape[0]\n",
    "    return risk_beta\n",
    "\n",
    "\n",
    "def gradient(Z, Y, Yhat):\n",
    "    \"\"\"\n",
    "    Calcuate the gradient of loss function at a point/row\n",
    "    Returns:\n",
    "    grad_: array(p, 1) gradient of funtion valuated at current beta value.\n",
    "    \"\"\"\n",
    "    grad_ = -Z.T.dot((Y - Yhat)) / Z.shape[0]\n",
    "    return grad_\n",
    "\n",
    "\n",
    "def gradient_descent(X, Y, beta, iters):\n",
    "    \"\"\"\n",
    "    Gradient Descent algorithm\n",
    "    Returns:\n",
    "    beta: array(p, 1) - logistic regression minimizer estimator\n",
    "    log_risk: a list of log loss at each step.\n",
    "    \"\"\"\n",
    "\n",
    "    N, p = X.shape  # Size of sample X\n",
    "    # Add column=1 to X for intercept\n",
    "    ones_column = np.ones((N, 1))\n",
    "    Z = np.hstack((ones_column, X))\n",
    "\n",
    "    # Check dimension of Y to reshape array(n,) to array(n, 1)\n",
    "    # if Y.ndim < 2:\n",
    "    #     Y = Y.reshape(-1, 1)\n",
    "\n",
    "    # Initialize dict to store parameters\n",
    "    hist = {}\n",
    "    hist[\"beta\"] = [beta.copy()]  # list to store beta value at each iteration\n",
    "    hist[\"risk\"] = [risk_beta(Z, Y, beta)]  # list to store risk value\n",
    "    hist[\"gradient\"] = []\n",
    "\n",
    "    # Execute loops\n",
    "    for iter in range(iters):\n",
    "\n",
    "        # calculate sigmoid fuction at current beta value\n",
    "        Yhat = sigmoid(Z, beta)\n",
    "\n",
    "        # learning rate\n",
    "        lr = 1 / (iter + 1)\n",
    "\n",
    "        # calculate gradient at current value\n",
    "        dbeta = gradient(Z, Y, Yhat)\n",
    "\n",
    "        # Update value of beta\n",
    "        beta = beta - lr * dbeta\n",
    "\n",
    "        # calculate risk at current beta value\n",
    "        risk = risk_beta(Z, Y, beta)\n",
    "\n",
    "        # Store values  of current iteration\n",
    "        hist[\"beta\"].append(beta.copy())\n",
    "        hist[\"risk\"].append(risk.copy())\n",
    "        hist[\"gradient\"].append(dbeta)\n",
    "\n",
    "    return beta, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, Y, beta, batch_size=10, iters=10):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent algorithm\n",
    "    Returns:\n",
    "    beta: array(p, 1) - logistic regression minimizer estimator\n",
    "    hist: a list of beta at each step.\n",
    "    \"\"\"\n",
    "    \n",
    "    N, p = X.shape  # Size of sample X\n",
    "\n",
    "    # Add column=1 to X for intercept\n",
    "    ones_column = np.ones((N, 1))\n",
    "    Z = np.hstack((ones_column, X))\n",
    "    # ZY = np.c_[Z, Y.reshape(N, 1)]\n",
    "\n",
    "    # Initialize dict to store parameters\n",
    "    hist = {}\n",
    "    hist[\"beta\"] = [beta.copy()]  # list to store beta value at each iteration\n",
    "    hist[\"risk\"] = [risk_beta(Z, Y, beta)]  # list to store risk value\n",
    "    hist[\"gradient\"] = []\n",
    "\n",
    "    # Main loops\n",
    "    for iter in range(iters):\n",
    "\n",
    "        # Shuffle data at the start of each iteration\n",
    "        # take a random batch of size batch_size\n",
    "        indices = np.random.choice(N, size=batch_size)\n",
    "        Z_batch = Z[indices]\n",
    "        Y_batch = Y[indices]\n",
    "\n",
    "        # calculate sigmoid fuction at current beta value\n",
    "        Yhat = sigmoid(Z_batch, beta)\n",
    "\n",
    "        # learning rate\n",
    "        lr = 1 / (iter + 1)\n",
    "\n",
    "        # calculate gradient at current value\n",
    "        dbeta = gradient(Z_batch, Y_batch, Yhat)\n",
    "\n",
    "        # Update value of beta\n",
    "        beta = beta - lr * dbeta\n",
    "\n",
    "        # calculate risk at current beta value\n",
    "        risk = risk_beta(Z, Y, beta)\n",
    "\n",
    "        # Store values  of current iteration\n",
    "        hist[\"beta\"].append(beta.copy())\n",
    "        hist[\"risk\"].append(risk.copy())\n",
    "        hist[\"gradient\"].append(dbeta)\n",
    "\n",
    "    return beta, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model\n",
    "\n",
    "We will use function `generate_data` from [Exercise 3](#exercise-3) to generate our test data X and Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train data\n",
    "X_train, Y_train = generate_data(seed=2796, n=3000, p=8)\n",
    "\n",
    "# Initialize parameters\n",
    "iter_gd = 100\n",
    "batch_size = 30\n",
    "iter_sgd = X_train.shape[0] * iter_gd / batch_size\n",
    "beta_init = np.zeros(X_train.shape[1] + 1)\n",
    "\n",
    "# Use Gradient Descent method\n",
    "beta_gd, hist_gd = gradient_descent(X=X_train, Y=Y_train, beta=beta_init, iters=int(iter_gd))\n",
    "\n",
    "# Use Stochastic Gradient Descent method\n",
    "beta_sgd, hist_sgd = stochastic_gradient_descent(X=X_train, Y=Y_train, beta=beta_init, batch_size=batch_size, iters=int(iter_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_beta(beta):\n",
    "    # Function to print beta coefficient\n",
    "\n",
    "    print('---Final optimized beta--- \\n')\n",
    "    print(f'{\"Intercept\":<15} {beta[0]:>10.6f}')\n",
    "    print(\"Coefficient:\")\n",
    "    for i, beta in enumerate(beta[1:]):\n",
    "        # This is to seperate beta and gamma coeff when we increase the degree of our model\n",
    "        if i <= 7:\n",
    "            print(f'beta_{i+1:<10} {beta:>10.6f} [x{i+1}]')\n",
    "        else: \n",
    "            print(f'gamma_{i-7:<9} {beta:>10.6f} [x{i-7}^2]')\n",
    "\n",
    "\n",
    "print('==== Gradient Descent ====')\n",
    "print_beta(beta_gd)\n",
    "print()\n",
    "print('==== Stochastic Gradient Descent ====')\n",
    "print_beta(beta_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the descent of log-loss function over our iteration. We notice that during first 6-7 iteration, the loss function decreases more rapidly, then it slows down. This is because we use a decay learning rate, so the adjustment we apply to $\\beta$ will be smaller after each iteration. This helps use avoid situation in which we jump pass the minimal point of function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over iterations\n",
    "# hist_risk_gd = risk_beta_hist(X_train, Y_train, hist_gd[\"beta\"])\n",
    "# hist_risk_sgd = risk_beta_hist(X_train, Y_train, hist_sgd[\"beta\"])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "\n",
    "# Plot the risk for Gradient Descent\n",
    "ax[0].plot(hist_gd[\"risk\"], \"o--\", color=\"navy\", markersize=3, label=\"GD\")\n",
    "ax[0].plot(hist_sgd[\"risk\"][:iter_gd], \"-\", color=\"r\", label=\"SGD (same nb of iterations)\")\n",
    "ax[0].set_xlabel(\"Iterations\")\n",
    "ax[0].set_ylabel(\"Risk R_beta\")\n",
    "ax[0].set_title(\"Risk over Iterations - GD\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the risk for Stochastic Gradient Descent\n",
    "ax[1].plot(hist_sgd[\"risk\"], \"-\", color=\"r\", label=\"SGD\")\n",
    "ax[1].set_xlabel(\"Iterations\")\n",
    "ax[1].set_ylabel(\"Risk R_beta\")\n",
    "ax[1].set_title(\"Risk over Iterations - SGD\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the trajectory of loss function using a contour plot. Since we have $p=8$ features, our parameter space is $\\beta \\in R^8$, we will only choose a $R^2$ space to project our function. We will shows contour plot in parameter space of 2 highest parameter values, which are $b_0$ and $b_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_risk_contour(ax, X, Y, beta, hist_gd, hist_sgd, b1_index, b2_index):\n",
    "    \"\"\"\n",
    "    Calculate risk w.r.t to b_0 and b_1\n",
    "    Keeping all other coefficient fix with optimized value.\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    Z = np.hstack((np.ones((N, 1)), X))\n",
    "    nb_iter = len(hist_gd[\"beta\"])  # take the number of iteration of GD\n",
    "\n",
    "    bgd = np.asarray(hist_gd[\"beta\"])\n",
    "    bsgd = np.asarray(hist_sgd[\"beta\"][:nb_iter])\n",
    "\n",
    "    # define a range of value for axis\n",
    "    min_beta1 = min(bgd[:, b1_index].min(), bsgd[:, b1_index].min())\n",
    "    max_beta1 = max(bgd[:, b1_index].max(), bsgd[:, b1_index].max())\n",
    "    min_beta2 = min(bgd[:, b2_index].min(), bsgd[:, b2_index].min())\n",
    "    max_beta2 = max(bgd[:, b2_index].max(), bsgd[:, b2_index].max())\n",
    "\n",
    "    range_b1 = np.linspace(min_beta1 - 0.1, max_beta1 + 0.1, 50)\n",
    "    range_b2 = np.linspace(min_beta2 - 0.1, max_beta2 + 0.1, 50)\n",
    "\n",
    "    B1, B2 = np.meshgrid(range_b1, range_b2)\n",
    "\n",
    "    # calculate grid of value for each value beta1, beta2\n",
    "    R = []\n",
    "    for b1, b2 in zip(np.ravel(B1), np.ravel(B2)):\n",
    "        beta[b1_index] = b1\n",
    "        beta[b2_index] = b2\n",
    "        R.append(risk_beta(Z, Y, beta))\n",
    "    R = np.array(R).reshape(B1.shape)\n",
    "\n",
    "    CS = ax.contourf(B1, B2, R, cmap=\"viridis\", alpha=0.3)  # contour space\n",
    "    plt.colorbar(CS, ax=ax)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_risk_trajectory(ax, hist_gd, hist_sgd, b1_index, b2_index):\n",
    "\n",
    "    nb_iter = len(hist_gd[\"beta\"])\n",
    "    bgd = np.asarray(hist_gd[\"beta\"])\n",
    "    bsgd = np.asarray(hist_sgd[\"beta\"][:nb_iter])\n",
    "\n",
    "    # Plot starting point\n",
    "    ax.plot(\n",
    "        bgd[:, b1_index][0],\n",
    "        bgd[:, b2_index][0],\n",
    "        \"^\",\n",
    "        label=\"start\",\n",
    "        color=\"red\",\n",
    "        markersize=8,\n",
    "    )\n",
    "\n",
    "    # beta trajectory of GD\n",
    "    ax.plot(\n",
    "        bgd[:, b1_index],\n",
    "        bgd[:, b2_index],\n",
    "        \"o-\",\n",
    "        lw=2,\n",
    "        alpha=0.5,\n",
    "        color=\"navy\",\n",
    "        label=\"GD\",\n",
    "    )\n",
    "\n",
    "    # beta trajectory of SGD\n",
    "    ax.plot(\n",
    "        bsgd[:, b1_index],\n",
    "        bsgd[:, b2_index],\n",
    "        \"+-\",\n",
    "        lw=2,\n",
    "        alpha=0.5,\n",
    "        color=\"red\",\n",
    "        label=\"SGD\",\n",
    "    )\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Risk of GD/SGD\\nprojected on (b0, b1) space\", loc=\"left\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
    "plot_risk_contour(ax[0], X, Y, beta_sgd, hist_gd, hist_sgd, 0, 1)\n",
    "plot_risk_trajectory(ax[0], hist_gd, hist_sgd, 0, 1)\n",
    "plot_risk_contour(ax[1], X, Y, beta_sgd, hist_gd, hist_sgd, 1, 2)\n",
    "plot_risk_trajectory(ax[1], hist_gd, hist_sgd, 1, 2)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Average Risk of GD/SGD\\nprojected on (b0, b1) space\", loc=\"left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomness of GD and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_simu = 10\n",
    "simu_gd = []\n",
    "simu_sgd = []\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(nb_simu):\n",
    "    _, simu_hist_gd = gradient_descent(X=X_train, Y=Y_train, beta=beta_init, iters=50)\n",
    "    simu_gd.append(simu_hist_gd[\"risk\"])\n",
    "    ax.plot(simu_hist_gd[\"risk\"], \"o--\", color=\"navy\", alpha=0.5, markersize=4, label=\"GD\")\n",
    "\n",
    "    _, simu_hist_sgd = stochastic_gradient_descent(X=X_train, Y=Y_train, beta=beta_init, batch_size=batch_size, iters=50)\n",
    "    simu_sgd.append(simu_hist_sgd[\"risk\"])\n",
    "    ax.plot(simu_hist_sgd[\"risk\"], \"-\", color=\"r\", alpha=0.5, label=\"SGD\")\n",
    "    \n",
    "    # plot_risk_trajectory(ax[1], simu_hist_gd, simu_hist_sgd, 0, 1)\n",
    "\n",
    " \n",
    "# plot_risk_contour(ax[1], X_train, Y_train, beta_sgd, hist_gd, hist_sgd, 0, 1)\n",
    "plt.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Now we will use our previous trained model to predict value of Y on a new test database. We will use misclassification rate as our model evaluation metric. The misclassification rate is defined as: \n",
    "\n",
    "$$\n",
    "Misc = \\hat{R_n}(\\beta) = \\frac{1}{n} \\sum(\\mathbb{1}_{Y_i \\neq h_{\\beta}(X_i)})\n",
    "$$\n",
    "\n",
    "with $h_{\\beta}(X_i)$ is our prediction of $Y$ given $X_i$, and is calculated based on the logit function. We will use $0.5$ as our decision boundary: \n",
    "\n",
    "$$\n",
    "h_{\\beta}(X_i) = \n",
    "\\begin{cases} \n",
    "    1 & \\text{if } q_\\beta(X_i) \\geq 0.5, \\\\\n",
    "    0 & \\text{if } q_\\beta(X_i) < 0.5.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Python implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, beta):\n",
    "    \"\"\"\n",
    "    Calculate prediction of trained model. \n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # Dict object to store results\n",
    "    predict = {}\n",
    "\n",
    "    # Calculate probability of Y\n",
    "    predict_prob_y = sigmoid(X, beta)\n",
    "    predict[\"predict_prob_y\"] = predict_prob_y\n",
    "\n",
    "    # Predict value of Y based on predict probability\n",
    "    predict_y = (predict_prob_y > 0.5).astype(int)\n",
    "    predict[\"predict_y\"] = predict_y\n",
    "\n",
    "    # Calculate the misclassification rate\n",
    "    misc_rate = (predict_y != Y).sum() / N\n",
    "    predict[\"misc_rate\"] = misc_rate\n",
    "\n",
    "    return predict\n",
    "\n",
    "# Generate test data\n",
    "N_test, p_test = 1000, 8\n",
    "X_test, Y_test = generate_data(seed=2258, n=N_test, p=p_test)\n",
    "\n",
    "# Add 1 column to feature data\n",
    "Z_test = np.hstack((np.ones((N_test, 1)), X_test))\n",
    "\n",
    "# Predict test data using previous trained model\n",
    "predict_gd = predict(Z_test, Y_test, beta_gd)\n",
    "predict_sgd = predict(Z_test, Y_test, beta_sgd)\n",
    "\n",
    "# Print the result\n",
    "print('==== Gradient Descent Test Result====')\n",
    "print(f\"Misclassification rate: {predict_gd[\"misc_rate\"]:.2%}\")\n",
    "print()\n",
    "print('==== Stochastic Gradient Descent Test Result====')\n",
    "print(f\"Misclassification rate: {predict_sgd[\"misc_rate\"]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the SGD is slightly better than GD in term of predicting Y value, but overall both methods deliver quite averaged result. Both methods have around 35% misclassification rate, which means we get around 65% accurated result, which is better than a random coin toss but obvjously far from a very good model. \n",
    "\n",
    "We can run simulation for several tests to see the average performance of both method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "nb_simu_test = 20\n",
    "misc_gd_ls = []\n",
    "misc_sgd_ls = []\n",
    "\n",
    "for i in range(nb_simu_test):\n",
    "    X_test_simu, Y_test_simu = generate_data(seed=i, n=N_test, p=p_test)\n",
    "    Z_test_simu = np.hstack((np.ones((N_test, 1)), X_test_simu))\n",
    "\n",
    "    predict_gd = predict(Z_test_simu, Y_test_simu, beta_gd)\n",
    "    predict_sgd = predict(Z_test_simu, Y_test_simu, beta_sgd)\n",
    "    \n",
    "    misc_gd_ls.append(predict_gd[\"misc_rate\"])\n",
    "    misc_sgd_ls.append(predict_sgd[\"misc_rate\"])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(misc_gd_ls, \"o--\", color=\"navy\", alpha=0.5, markersize=3, label=\"GD\")\n",
    "ax.plot(misc_sgd_ls, \"-\", color=\"r\", alpha=0.5, label=\"SGD\")\n",
    "\n",
    "ax.set_title(\"Misclassification Rates for GD and SGD\")\n",
    "ax.set_xlabel(\"simulation\")\n",
    "ax.set_ylabel(\"Misclassification Rate\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that during our simulation, the misclassification rates are mostly fall into range of 25% to 35%. This outcome is expected since we know our true logit function has a degree of 2, while we have chosen our model to have a degree of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Now we consider more complexed model, with degree 2 - which is closer to our true function: \n",
    "\n",
    "$$\n",
    "q_{\\beta, \\gamma}(x) = \\frac{\\exp(\\beta_0 + \\beta^T x + \\gamma^T x^2)}{1 + \\exp(\\beta_0 + \\beta^T x + \\gamma^T x^2)}\n",
    "$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^p \\to x^2 \\in \\mathbb{R}^p$ is applied component-wise. We notice that: \n",
    "\n",
    "- Now for each iteration of GD and SGD, two vectors need to be updated: $\\beta \\in \\mathbb{R}^{p+1}$ (include the intercept) and $\\gamma \\in \\mathbb{R}^p$\n",
    "\n",
    "- $\\beta$ and $\\gamma$ are independent coefficient terms appli linearly to the input vector. \n",
    "\n",
    "- The derivative with respect to $\\beta$ will be the same as before. \n",
    "\n",
    "- The derivative with respect to $\\gamma$ will have similar mathematic formula, execept that we will use $x^2$ instead of $x$ and we will not include the constant for intercept. \n",
    "\n",
    "- We need to update all other functions ton include the quadratic term. \n",
    "\n",
    "**Python implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_quad(X, Y, beta, batch_size=10, iters=10):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent algorithm\n",
    "    With Quadratic term\n",
    "    Returns:\n",
    "    beta: array(p+1, 1) - logistic regression minimizer estimator for x (degree 1)\n",
    "    gamma: array(p, 1) - logictic regression minimizer estimator for x^2 (degree 2)\n",
    "    hist: a dict object that stores values at each step\n",
    "    \"\"\"\n",
    "    \n",
    "    N, p = X.shape  # Size of sample X\n",
    "\n",
    "    # Add column=1 to X for intercept\n",
    "    ones_column = np.ones((N, 1))\n",
    "    X2 = X ** 2\n",
    "    Z = np.hstack((ones_column, X, X2))\n",
    "    # ZY = np.c_[Z, Y.reshape(N, 1)]\n",
    "\n",
    "    # Initialize dict to store parameters\n",
    "    hist = {}\n",
    "    hist[\"beta\"] = [beta[:p+2].copy()]  # list to store beta value at each iteration\n",
    "    hist[\"gamma\"] = [beta[p+2:].copy()]  # list to store beta value at each iteration\n",
    "    hist[\"risk\"] = [risk_beta(Z, Y, beta)]  # list to store risk value\n",
    "    hist[\"gradient\"] = []\n",
    "\n",
    "    # Main loops\n",
    "    for iter in range(iters):\n",
    "\n",
    "        # Shuffle data at the start of each iteration\n",
    "        # take a random batch of size batch_size\n",
    "        indices = np.random.choice(N, size=batch_size)\n",
    "        Z_batch = Z[indices]\n",
    "        Y_batch = Y[indices]\n",
    "\n",
    "        # calculate sigmoid fuction at current beta value\n",
    "        Yhat = sigmoid(Z_batch, beta)\n",
    "\n",
    "        # learning rate\n",
    "        lr = 1 / (iter + 1)\n",
    "\n",
    "        # calculate gradient at current value\n",
    "        dbeta = gradient(Z_batch, Y_batch, Yhat)\n",
    "\n",
    "        # Update value of beta\n",
    "        beta = beta - lr * dbeta\n",
    "\n",
    "        # calculate risk at current beta value\n",
    "        risk = risk_beta(Z, Y, beta)\n",
    "\n",
    "        # Store values  of current iteration\n",
    "        hist[\"beta\"].append(beta[:p+2].copy())\n",
    "        hist[\"gamma\"].append(beta[p+2:].copy())\n",
    "        hist[\"risk\"].append(risk.copy())\n",
    "        hist[\"gradient\"].append(dbeta)\n",
    "\n",
    "    # Results\n",
    "    return beta, hist\n",
    "\n",
    "\n",
    "# We will use the same X_train, Y_train, X_test, Y_test dataset as before. \n",
    "# Run sgd_quad with the same number of iteration as the previous sgd\n",
    "beta_init = np.zeros(X_train.shape[1] * 2 + 1)\n",
    "beta_gamma_sgd_quad, hist_sgd_quad = sgd_quad(X=X_train, Y=Y_train, beta=beta_init, batch_size=batch_size, iters=int(iter_sgd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print('==== Stochastic Gradient Descent (Quadratic model) ====')\n",
    "print(f\"Number of iterations: {iter_sgd:_.0f}\")\n",
    "print_beta(beta_gamma_sgd_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the improvement of SGD when considering more complexed model:\n",
    "\n",
    "- SGD has correctly reduce the coefficeint of the intercept to a near-zero value. Previously, the SGD result for model 1 has set intercept equal 0.617282, which is as large as `beta_1`. \n",
    "\n",
    "- The two largest coefficients are `beta_1` (corresponding to $x_1$) and `gamma_2` (corresponding to $x_2^2$). We know that our true logit function for $q_{x}$ is: \n",
    "\n",
    "$$p(x) = \\frac{\\exp(x_1 + x_2^2 )}{1 + \\exp(x_1 + x_2^2 )}$$\n",
    "\n",
    "So, with the same number of iterations (which means same number of evaluation), SGD for model 2 has correctly chosen the important features in our dataset.\n",
    "\n",
    "We now test SGD for model 2 with the same test data `X_test`, `Y_test` and measure its performance using misclassification metric (the lower the misclassification rate, the better the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to include 1 column and quadratic term\n",
    "X_test_squared = X_test ** 2\n",
    "Z_test_poly = np.hstack((np.ones((N_test, 1)), X_test, X_test_squared))\n",
    "\n",
    "# Predict test data using previous trained model\n",
    "predict_sgd_quad = predict(Z_test_poly, Y_test, beta_gamma_sgd_quad)\n",
    "\n",
    "# Print the result\n",
    "print('==== Stochastic Gradient Descent (Model 1) Test Result====')\n",
    "print(f\"Misclassification rate: {predict_sgd[\"misc_rate\"]:.2%}\")\n",
    "print()\n",
    "print('==== Stochastic Gradient Descent (Model 2) Test Result====')\n",
    "print(\"with quadratic term\")\n",
    "print(f\"Misclassification rate: {predict_sgd_quad[\"misc_rate\"]:.2%}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our test data, we see that even though Model 2 performs better than Model 1, our accuracy does not increase significantly. We will try to run simulation several time with difference test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "nb_simu_test = 20\n",
    "misc_sgd_model1 = []\n",
    "misc_sgd_model2 = []\n",
    "\n",
    "for i in range(nb_simu_test):\n",
    "    X_test_simu, Y_test_simu = generate_data(seed=i, n=N_test, p=p_test)\n",
    "    X_test_simu_squared = X_test_simu**2\n",
    "    Z_test_simu = np.hstack((np.ones((N_test, 1)), X_test_simu))\n",
    "    Z_test_poly_simu = np.hstack(\n",
    "        (np.ones((N_test, 1)), X_test_simu, X_test_simu_squared)\n",
    "    )\n",
    "\n",
    "    predict_sgd_model1 = predict(Z_test_simu, Y_test_simu, beta_sgd)\n",
    "    predict_sgd_model2 = predict(Z_test_poly_simu, Y_test_simu, beta_gamma_sgd_quad)\n",
    "\n",
    "    misc_sgd_model1.append(predict_sgd_model1[\"misc_rate\"])\n",
    "    misc_sgd_model2.append(predict_sgd_model2[\"misc_rate\"])\n",
    "\n",
    "    avg_misc_model1 = np.mean(misc_sgd_model1)\n",
    "    avg_misc_model2 = np.mean(misc_sgd_model2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot result for Model1\n",
    "ax.plot(misc_sgd_model1, \"--\", color=\"blue\", alpha=0.5, label=\"SGD-Model1\")\n",
    "ax.axhline(\n",
    "    avg_misc_model1, linestyle=\"-\", color=\"blue\", alpha=1, label=\"AVG-SGD-Model1\"\n",
    ")\n",
    "ax.text(\n",
    "    0.5,\n",
    "    avg_misc_model1 - 0.005,\n",
    "    f\"{avg_misc_model1:.2f}\",\n",
    "    ha=\"center\",\n",
    "    color=\"blue\",\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "# Plot result for Model2\n",
    "\n",
    "ax.plot(misc_sgd_model2, \"*--\", color=\"r\", alpha=0.6, label=\"SGD-Model2\")\n",
    "ax.axhline(avg_misc_model2, linestyle=\"-\", color=\"r\", alpha=1, label=\"AVG-SGD-Model2\")\n",
    "ax.text(\n",
    "    0.5,\n",
    "    avg_misc_model2 - 0.005,\n",
    "    f\"{avg_misc_model2:.2f}\",\n",
    "    ha=\"center\",\n",
    "    color=\"r\",\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "ax.set_title(\"Misclassification Rates for SGD 2 models\")\n",
    "ax.set_xlabel(\"simulation\")\n",
    "ax.set_ylabel(\"Misclassification Rate\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our simulation, we observed that Model 2 always outperforms Model 1. On average, Model 2 achieves a 27% misclassification rate, while Model 1 has an average misclassification rate of around 31%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Now we will use logistic Lasso from the `scikit-learn` package to train Model1 and Model2. \n",
    "\n",
    "**Data generation**\n",
    "\n",
    "1. Previously, we generated train data set and test data set seperately. With `scikit-learn`, to follow general convention, we will create new data set and use `train_test_split` function. Our new dataset will have 5000 observations, with 80% will be used for training and 20% for testing. \n",
    "\n",
    "2. Our current 2 models are: \n",
    "\n",
    "- Model 1: only include linear term of features\n",
    "\n",
    "$$\n",
    "x_i = \n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & ... & x_8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Model 2: we add quadratic term of features to Model 1\n",
    "\n",
    "$$\n",
    "x_i = \n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & ... & x_8 & x_1^2 & x_2^2 & ... & x_8^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Additionally, `sklearn` has a `PolynomialFeatures` that generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the given degree [(sklearn poly-feature)](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). We will create our Model 3 wih this also for comparison. \n",
    "\n",
    "In our example, our model will be: \n",
    "\n",
    "$$\n",
    "x_i = \n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & ... & x_8 & x_1^2 & x_1 x_2 & x_2^2 & ... & x_7 x_8 & x_8^2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and preprocessing data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, Y = generate_data(seed=100, n=5000, p=8)\n",
    "\n",
    "# Model 1 Data: \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
    "\n",
    "# Model 2 Data: \n",
    "X_train_2, X_test_2 = np.hstack((X_train, X_train ** 2)), np.hstack((X_test, X_test ** 2))\n",
    "\n",
    "# Model 3 Data: \n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_3, X_test_3 = poly.fit_transform(X_train), poly.fit_transform(X_test)\n",
    "\n",
    "# Final dataset\n",
    "X_trains = (X_train, X_train_2, X_train_3)\n",
    "X_tests = (X_test, X_test_2, X_test_3)\n",
    "\n",
    "for i, (X_train_, X_test_) in enumerate(zip(X_trains, X_tests), start=1):\n",
    "    print(f\"Model {i}\")\n",
    "    print(f\"X_train shape: {X_train_.shape}\")\n",
    "    print(f\"X_test shape: {X_test_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train models using Logistic LASSO**\n",
    "\n",
    "`scikit-learn` provides `LogisticRegressionCV` class that perform logistic regression with built-in cross-validation to optimize parameter $\\lambda$.\n",
    "\n",
    "The value for $\\lambda$ is set by parameter `Cs` and can be accessed by model attribute `C_`. Each of the values in `Cs` describes the inverse of regularization strength. If `Cs` is as an int, then a grid of Cs values are chosen in a logarithmic scale between $10^{-4}$ and $10^{4}$. Note that since this is the inverse of $\\lambda$, smaller values of `C_` means larger value for $\\lambda$, which specifies stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "\n",
    "models = []\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Use LogisticRegressionCV\n",
    "for i, (X_train_, X_test_) in enumerate(zip(X_trains, X_tests), start=1):\n",
    "    model = LogisticRegressionCV(\n",
    "        Cs=10, cv=10, max_iter=1000, random_state=123, penalty=\"l1\", solver=\"liblinear\", scoring='accuracy'\n",
    "    ).fit(X_train_, Y_train)\n",
    "    models.append(model)\n",
    "    RocCurveDisplay.from_estimator(model, X_test_, Y_test, ax=ax, name=f\"Model{i}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized value for regularization parameter\")\n",
    "print(\"LogisticRegressionCV\")\n",
    "print(\"==============\")\n",
    "for i, model in enumerate(models, start=1): \n",
    "    print(f\"Model {i}: {model.C_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define parameter grid to fine tune alpha value\n",
    "# use 10 values from 1e-4 to 1e4\n",
    "param_grid = {\"alpha\": np.logspace(-4, 4, 10)}\n",
    "\n",
    "# SGD Classifier\n",
    "sgd_clf = SGDClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    penalty=\"l1\",\n",
    "    random_state=100,\n",
    "    max_iter=1000,\n",
    "    learning_rate=\"optimal\",\n",
    ")\n",
    "\n",
    "\n",
    "models_sgd = []\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Using GridsearchCV for 3 models\n",
    "for i, (X_train_, X_test_) in enumerate(zip(X_trains, X_tests), start=1):\n",
    "    model = GridSearchCV(sgd_clf, param_grid, cv=10, scoring=\"accuracy\").fit(\n",
    "        X_train_, Y_train\n",
    "    )\n",
    "    models_sgd.append(model)\n",
    "    RocCurveDisplay.from_estimator(\n",
    "        model.best_estimator_, X_test_, Y_test, ax=ax, name=f\"SGDClassifier-Model{i}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized value for regularization parameter\")\n",
    "print(\"SGDClassifier\")\n",
    "print(\"==============\")\n",
    "\n",
    "for i, model in enumerate(models_sgd, start=1):\n",
    "    print(f\"Model {i}: {model.best_params_['alpha']:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import and cleaning\n",
    "\n",
    "First we will import the data directly from url .\n",
    "\n",
    "Next, we will perform some basic initial data analysis to gain insight into our dataset. Some simple questions we try to address such as: \n",
    "\n",
    "- How many observations in the data?\n",
    "\n",
    "- What will be our feature columns and target column(s). Other columns will be removed. \n",
    "\n",
    "- Is there any missing data? Is there any duplicated data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load data\n",
    "url = \"https://www.dropbox.com/scl/fi/9uz7mu64ew91go651qn11/spam.csv?rlkey=rp07p7kmt27gox1c3sckeehdo&e=1&st=yky8ql7n&dl=1\"\n",
    "data = pd.read_csv(url, encoding='ISO-8859-1')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep the first 2 columns\n",
    "data = data.iloc[:, :2]\n",
    "data = data.rename(columns={'v1': 'label', 'v2':'text'})\n",
    "\n",
    "# Encode spam data as 1 and ham data as 0\n",
    "data['spam']=data['label'].apply(lambda x:1 if x=='spam' else 0)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated data\n",
    "data.duplicated().sum()\n",
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated data\n",
    "data = data.drop_duplicates(keep='first')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning, our data set has 5169 rows (observations) with our feature column is `text` and our targeted column is `spam`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take spam column as our targeted Y array\n",
    "Y = data['spam'].copy().to_numpy()\n",
    "print(Y.shape)\n",
    "print(Y)\n",
    "\n",
    "\n",
    "# take text column as our feature X array\n",
    "X = data['text'].copy().to_numpy()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "Now we will perform some explonatory data analysises to see if there're some important features that can be used to separate spam email from normal email. Since we are working with text classification, some characteristics (features) of the text we can use are:\n",
    "\n",
    "- The number of characters in the text. \n",
    "\n",
    "- The number of words in the text.\n",
    "\n",
    "- The number of sentences in the text.\n",
    "\n",
    "- The number of uppercase words in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data_eda = data.copy()\n",
    "\n",
    "# calculate basic features of text\n",
    "# number of characters\n",
    "data_eda[\"nb_character\"] = data_eda[\"text\"].apply(lambda x: len(x))\n",
    "\n",
    "# number of capitalized words\n",
    "data_eda[\"nb_cap\"] = data_eda[\"text\"].apply(\n",
    "    lambda x: sum(1 for word in x.split() if word.isupper())\n",
    ")\n",
    "\n",
    "# number of words\n",
    "data_eda[\"nb_word\"] = data_eda[\"text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# number of sentences\n",
    "data_eda[\"nb_sentence\"] = data_eda[\"text\"].apply(\n",
    "    lambda x: len([sent for sent in re.split(r\"[.!?]+\", x) if sent.strip()])\n",
    ")\n",
    "\n",
    "data_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eda.groupby('spam').describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of spam in our data set**\n",
    "\n",
    "We see that in our dataset, we have more normal data than spam data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_eda['spam'])\n",
    "plt.title('Distribution of spam/ham emails')\n",
    "plt.xlabel('Email label (0-ham 1-spam)')\n",
    "plt.ylabel('Nb. of emails')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of features for spam/normal emails**\n",
    "\n",
    "Using the same method as previously in [Exercise 3](#exercise-3), we will plot the distribution of each feature against the label (`Y=0` or `Y=1`). The result shows that we obtained some quite distinctions between `spam` and `ham` email just by using some basic characteristics of the text, i.e: the number of characters, number of words, etc.... For example, normal emails on avrage has more characters/words than spam emails, as we can see a quite good boundary decision between spam and ham email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "feature_cols = ['nb_character', 'nb_word', 'nb_cap', 'nb_sentence']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, f in enumerate(feature_cols):\n",
    "\n",
    "    data_f = data_eda[f]\n",
    "    sns.kdeplot(data_f[data_eda['spam'] == 0], ax=ax[i], color=\"red\", fill=True, label=\"spam\")\n",
    "    sns.kdeplot(data_f[data_eda['spam'] == 1], ax=ax[i], color=\"blue\", fill=True, label=\"ham\")\n",
    "\n",
    "    ax[i].set_title(f\"Distribution of {f}\")\n",
    "\n",
    "# Show the plot\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=2, fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairplot between features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot between features\n",
    "sns.set_theme(style='ticks', color_codes=True)\n",
    "g = sns.pairplot(data_eda, hue='spam', diag_kind='kde', markers=[\"o\", \"s\"])\n",
    "\n",
    "g.figure.suptitle(\"Pairplot of features by spam/ham email\")\n",
    "plt.subplots_adjust(top=0.95)  # Adjust the position of the title\n",
    "\n",
    "g._legend.set_title('email')\n",
    "for t, l in zip(g._legend.texts, ['spam', 'ham']):\n",
    "    t.set_text(l)\n",
    "\n",
    "# Show the pairplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Generate count matrix\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "print(X.shape)\n",
    "\n",
    "# Apply TF-IDF transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tf = tfidf_transformer.fit_transform(X)\n",
    "print(X_tf.__class__)\n",
    "print(X_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with LASSO and Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split train and test data\n",
    "X_train, Y_train, X_test, Y_test = train_test_split(\n",
    "    X_tf, Y, random_state=25, train_size=0.8\n",
    ")\n",
    "\n",
    "# Apply LASSO classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark fromm profesor\n",
    "\n",
    "### Question 3:\n",
    "\n",
    "We have nonlinearity which differs from standard logistic. \n",
    "Loop n = 3000 with each x is a Gaussian N(o, I)\n",
    "x_i in R^8\n",
    "x_i are independent of each other because of the Identity matrix. \n",
    "We can also use for loop. \n",
    "\n",
    "### Question 4: \n",
    "\n",
    "- We will have large bias because of the nonlinearity. Maybe include a computation and graph of variane of the predictors. \n",
    "- Remarks: there is an intercept. number of parameter is p = 8 + 1 = 9.\n",
    "- Give the formula to GD (explainations, math derivatives, well written, update rule)\n",
    "- Define Z as: 1 column + X. \n",
    "- In order to get GD, we need: \n",
    "    - Gradient. We will divided by n to have something more like a gradient. \n",
    "    - Learning rate: 1/K with K is the number of iterations.\n",
    "    - Initialize value of beta. Choose random value from standard gaussian or 0. \n",
    "    - Update rule. beta^k+1 = beta^k - gamma_k+1 * gradient with k > 0.\n",
    "- SGD:\n",
    "    - the point in exam is to consider general problems where the computing time is measured with respected to gradient evaluation. \n",
    "    specially number of time sigmoid(beta_i) is evaluated. \n",
    "    - We dont want to focus on logistic regression where some benefit are observed easily using vectorize operation.\n",
    "    We can show which method decrease the Risk faster. \n",
    "    - Implementation: fix the number of iterations: K = 100 for GD. \n",
    "    For each stage: we need n evaluation of q_beta. \n",
    "    Do it K time. \n",
    "    cardinality of q_beta = 300_000. \n",
    "    - Choose minibatch size = m = 30. (include mini_batch size in SGD function)\n",
    "    - We need 30 evaluation at each SGD iterations. \n",
    "    R_m^{SGD} = 1/m \\sum (Y_i - q_beta(X_i)) * Z_i with Z_i = column(1 X_i)\n",
    "    - So in the end, after \\tilt{K} we have: \\tilt{K}*m = Kn.\n",
    "    We need \\tilt{K} = Kn/m = 10_000.\n",
    "    We dont need to be precise at the beginning of our iterations. The advantage of SGD over GD. \n",
    "    We dont need to use epoch in our code. \n",
    "\n",
    "### Question 5: \n",
    "- Just look at misclassification rate: propotion of wrong prediction. \n",
    "\n",
    "\\hat{R_n}(given beta) = 1/n \\sum(Indicator{Y_i <> h_beta(X_i)})\n",
    "\n",
    "with h_beta(X) = 1 if q_beta(X) >= 1/2\n",
    "= 0 if q_Beta(X) < 1/2. \n",
    "\n",
    "Or we can use Bernoulli(q_beta(X))\n",
    "\n",
    "- Generate new sample for evaluation. (n=1000 sample size)\n",
    "- For real data, we need to use test train spit for cross validation. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
